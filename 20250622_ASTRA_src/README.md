ì¢‹ìŠµë‹ˆë‹¤. ìš”ì²­í•˜ì‹  **VadCLIP ìŠ¤íƒ€ì¼ì˜ README.md** í˜•ì‹ìœ¼ë¡œ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ ASTRA ë²„ì „ì„ ì •ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

---

# ASTRA

This is the official PyTorch implementation of our system:
**"ASTRA: Multimodal Weakly-Supervised Video Anomaly Detection Framework"**

![framework](data/framework.png)

---

## ğŸ” Highlight

* We present **ASTRA**, a novel **multimodal weakly-supervised anomaly detection framework** that integrates **visual, audio, and text modalities** to automatically detect abnormal events in long video streams.
* With **Cross-Modal Attention (CMA)**, ASTRA selectively fuses complementary audioâ€“visual cues, overcoming the limitations of unimodal detectors.
* A **Localâ€“Global Temporal (LGT) module** leverages both Transformers and Graph Convolution Networks (Similarity & Distance graphs) to capture short- and long-range dependencies.
* A new **Contrastive Multi-modal Alignment Loss (CMAL)** based on InfoNCE aligns audioâ€“visual representations under weak supervision, enabling precise anomaly localization without frame-level labels.
* **CLIP-based text prompt alignment** grounds anomaly categories (e.g., *fighting, shooting, explosion*) into the semantic space, providing **zero-shot adaptability** to unseen anomaly types.
* The system achieves robust **video-level classification** and **frame-level localization**, while significantly reducing annotation costs compared to fully-supervised methods.

---

## âš™ï¸ Environment Setup

We provide a Conda environment file for reproducibility:

```bash
conda env create -f environment.yml
conda activate astra
```

Key dependencies include:

* Python 3.10
* PyTorch 1.13.0, Torchvision 0.14.0, Torchaudio 0.13.0
* timm==0.6.7, einops, fvcore, scikit-learn, faiss-cpu, opencv-python
* transformers==4.31.0, accelerate, pytorchvideo

---

## ğŸ“Š Data Preparation

1. Extract **visual features** (512-d, CLIP/I3D) and **audio features** (128-d, VGGish).
2. Save them in `.npy` format.
3. Prepare CSV list files with feature paths and labels:

   * `list/xd_CLIP_rgb.csv`
   * `list/xd_CLIP_audio.csv`
   * `list/xd_CLIP_rgbtest.csv`
   * `list/xd_CLIP_audiotest.csv`

**Label mapping:**

```
A   : normal
B1  : fighting
B2  : shooting
B4  : riot
B5  : abuse
B6  : car accident
G   : explosion
```

---

## ğŸš€ Training and Testing

After setup, run the following commands:

**XD-Violence dataset**

```bash
python xd_train.py
python xd_test.py
```

**UCF-Crime dataset** (example extension)

```bash
python ucf_train.py
python ucf_test.py
```

---

## ğŸ“‚ Project Structure

```
ASTRA/
â”œâ”€â”€ model.py             # Main model (CLIPVAD, SingleModel)
â”œâ”€â”€ layers.py            # Graph Convolution & Attention layers
â”œâ”€â”€ CMA_MIL.py           # Contrastive Multi-modal Alignment Loss
â”œâ”€â”€ InfoNCE.py           # InfoNCE contrastive loss
â”œâ”€â”€ dataset.py           # Dataset loader (XDDataset, UCFDataset)
â”œâ”€â”€ xd_train.py          # Training pipeline
â”œâ”€â”€ xd_test.py           # Testing & evaluation
â”œâ”€â”€ xd_option.py         # Argument parser & hyperparameters
â”œâ”€â”€ xd_detectionMAP.py   # mAP computation module
â”œâ”€â”€ tools.py             # Utility functions
â”œâ”€â”€ environment.yml      # Conda environment
â””â”€â”€ README.md
```

---

## ğŸ“š References

We referenced the following repositories during implementation:

* [XDVioDet](https://github.com/Roc-Ng/XDVioDet)
* [DeepMIL](https://github.com/Roc-Ng/DeepMIL)

---

## ğŸ“œ Citation

If you find this repository useful for your research, please consider citing:

```bibtex
